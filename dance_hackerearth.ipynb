{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dance_hackerearth.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eAIopLuZIR_q",
        "eNBugZRJIYtn"
      ],
      "mount_file_id": "1IAzaRQl2rgBQMmOaSPa-zxziZJtUiCox",
      "authorship_tag": "ABX9TyPwW1YZr680ukLk4pReaKJd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1d13ff0d528c48af956330ad1791433a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_be697b20e6be403592a2932ff6ea1c73",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c32d21e932614a11ae29e17ae0927094",
              "IPY_MODEL_61d6cb7117884acba4a30bcaf4696f18"
            ]
          }
        },
        "be697b20e6be403592a2932ff6ea1c73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c32d21e932614a11ae29e17ae0927094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_67e9662bcf0a4588a0b350e122520c56",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 39,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 39,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7ffa1cecd69d40cc83edff9e4ed6b32f"
          }
        },
        "61d6cb7117884acba4a30bcaf4696f18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2751ada90461401187102e0efbf378c4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 39/39 [00:06&lt;00:00,  6.06it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d13c6c1ae6e5479c807aae850be709f2"
          }
        },
        "67e9662bcf0a4588a0b350e122520c56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7ffa1cecd69d40cc83edff9e4ed6b32f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2751ada90461401187102e0efbf378c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d13c6c1ae6e5479c807aae850be709f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utsavnandi/hackerearth-dance/blob/master/dance_hackerearth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs-9oRSrIIsD",
        "colab_type": "text"
      },
      "source": [
        "## Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQ8TPLs9h9so",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U git+https://github.com/albu/albumentations -q\n",
        "!pip install -U git+https://github.com/rwightman/pytorch-image-models -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8RWvWglio7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the datafiles must be unzipped into ./data/ folder\n",
        "# !unzip drive/My\\ Drive/dance.zip -d ./data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xNPf84rIMvE",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VD07YqyicL1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "224ae50c-654c-4383-bfdb-d4d10b8bfc5d"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "from getpass import getpass\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import auth\n",
        "from google.cloud import storage\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torch.cuda.amp import GradScaler\n",
        "import torchvision\n",
        "\n",
        "import timm\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "#import neptune\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed = 43\n",
        "seed_everything(seed)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_OJNgKWijpK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = './data/dataset/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgbm9Q6jjfsz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c90f47e-aab7-4ef5-8ecc-5dad0483c291"
      },
      "source": [
        "df_train = pd.read_csv(DATA_DIR+'train.csv')\n",
        "df_test = pd.read_csv(DATA_DIR+'test.csv')\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>96.jpg</td>\n",
              "      <td>manipuri</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>163.jpg</td>\n",
              "      <td>bharatanatyam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>450.jpg</td>\n",
              "      <td>odissi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>219.jpg</td>\n",
              "      <td>kathakali</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>455.jpg</td>\n",
              "      <td>odissi</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Image         target\n",
              "0   96.jpg       manipuri\n",
              "1  163.jpg  bharatanatyam\n",
              "2  450.jpg         odissi\n",
              "3  219.jpg      kathakali\n",
              "4  455.jpg         odissi"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgwRRaSYur-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "le = LabelEncoder()\n",
        "df_train['target'] = le.fit_transform(df_train['target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbiPUDl5jvQd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2192e8d0-9e40-45e2-c426-53a144cfc3a6"
      },
      "source": [
        "df_train['target'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5    50\n",
              "6    49\n",
              "2    47\n",
              "0    47\n",
              "3    46\n",
              "7    45\n",
              "1    44\n",
              "4    36\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAIopLuZIR_q",
        "colab_type": "text"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOlCm7qzkcuR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DanceDataset(Dataset):\n",
        "    def __init__(self, df, isEval=True, transform=None):\n",
        "        super().__init__()\n",
        "        self.image_id = df['Image'].values\n",
        "        self.transform = transform\n",
        "        self.isEval = isEval\n",
        "        if not self.isEval:\n",
        "            self.labels = df['target'].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_id)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "        \n",
        "        if self.isEval:\n",
        "            image_name = DATA_DIR + f'test/{self.image_id[index]}'\n",
        "        else:\n",
        "            image_name = DATA_DIR + f'train/{self.image_id[index]}'\n",
        "\n",
        "        image = cv2.imread(image_name, cv2.IMREAD_COLOR).astype(np.uint8)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.uint8)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image=image)['image']\n",
        "        else:\n",
        "            image = image.astype(np.float32)\n",
        "            \n",
        "        if self.isEval:\n",
        "            return image\n",
        "\n",
        "        target = self.labels[index].astype(np.int64)\n",
        "        return image, target\n",
        "\n",
        "def get_datasets(labels_train, labels_test):\n",
        "    datasets = {}\n",
        "    datasets['train'] = DanceDataset(\n",
        "        labels_train, isEval=False, transform=get_train_transform()\n",
        "    )\n",
        "    datasets['valid'] = DanceDataset(\n",
        "        labels_test, isEval=False, transform=get_valid_transform()\n",
        "    )\n",
        "    return datasets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PS9QlxdIUL8",
        "colab_type": "text"
      },
      "source": [
        "## Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLqezRTNmPlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_SIZE = 300\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "\n",
        "def get_train_transform(p=1.0):\n",
        "    return A.Compose([\n",
        "        A.Resize(IMG_SIZE, IMG_SIZE, interpolation=2, always_apply=True, p=1),\n",
        "        A.CenterCrop(int(IMG_SIZE/2), int(IMG_SIZE/2), p=0.25),\n",
        "        A.Resize(IMG_SIZE, IMG_SIZE, interpolation=2, always_apply=True, p=1),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.ShiftScaleRotate(\n",
        "            interpolation=2,\n",
        "            shift_limit=0.2, scale_limit=0.2, \n",
        "            rotate_limit=15, p=0.3\n",
        "        ),\n",
        "        A.HueSaturationValue(\n",
        "            hue_shift_limit=20, sat_shift_limit=30, \n",
        "            val_shift_limit=20, p=0.3\n",
        "        ),\n",
        "        A.Normalize(mean, std, max_pixel_value=255.0, always_apply=True),\n",
        "        ToTensorV2(p=1.0),\n",
        "    ], p=p)\n",
        "\n",
        "def get_valid_transform():\n",
        "    return A.Compose([\n",
        "        A.Resize(IMG_SIZE, IMG_SIZE, interpolation=2, always_apply=True, p=1),\n",
        "        A.Normalize(mean, std, max_pixel_value=255.0, always_apply=True),\n",
        "        ToTensorV2(p=1.0),\n",
        "    ])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_zPvlbTIW31",
        "colab_type": "text"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yotcyadHtWLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class ResNet18(nn.Module): \n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = torchvision.models.resnet18(pretrained=True)\n",
        "        in_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(in_features, 8)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, name='tf_efficientnet_b0'):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(name, pretrained=True)\n",
        "        in_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Linear(in_features, 8)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNBugZRJIYtn",
        "colab_type": "text"
      },
      "source": [
        "## Loss and Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3deyBkPw5jS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def criterion(y_pred, y_true):\n",
        "    return nn.CrossEntropyLoss()(y_pred, y_true)\n",
        "\n",
        "def get_f1(y_pred, y_true):\n",
        "    return f1_score(y_true.cpu().numpy(), y_pred.cpu().numpy(), average='weighted')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r3eEbFOIcSG",
        "colab_type": "text"
      },
      "source": [
        "## Train script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z2PhUp_xuSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "def train_one_epoch(loader, model, optimizer, scheduler=None, log=False):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for idx, (images, targets) in enumerate(loader):\n",
        "        images = images.to(device)\n",
        "        targets = targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(images.float())\n",
        "        loss = criterion(y_pred, targets)\n",
        "        running_loss += float(loss.item())\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        \n",
        "        if (idx+1) % FLAGS['log_interval'] == 0 and log==True:\n",
        "            print(\"Loss/train\", float(loss))\n",
        "\n",
        "    return running_loss/len(loader)\n",
        "\n",
        "def val_one_epoch(loader, model):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_f1 = 0.0\n",
        "    with torch.no_grad():\n",
        "        for idx, (images, targets) in enumerate(loader):\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "            y_pred = model(images.float())\n",
        "            loss = criterion(y_pred, targets)\n",
        "            running_loss += float(loss)\n",
        "            running_f1 += float((get_f1(y_pred.argmax(1).float(), targets)).mean())\n",
        "\n",
        "    return running_loss/len(loader), running_f1/len(loader)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqlA29lDzQq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def fit(labels_train, labels_test, log=False):\n",
        "    global FLAGS\n",
        "    best_score = 0.0\n",
        "\n",
        "    #model = ResNet18().to(device)\n",
        "    model = Model(FLAGS['model_name']).to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(), \n",
        "        lr=FLAGS['learning_rate'], \n",
        "        weight_decay=FLAGS['weight_decay']\n",
        "    )\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, 'max', factor=0.5, verbose=True, patience=2\n",
        "    )\n",
        "    \n",
        "    datasets = get_datasets(labels_train, labels_test)\n",
        "    \n",
        "    #loaders\n",
        "    train_loader = DataLoader(\n",
        "        datasets['train'], batch_size=FLAGS['batch_size'], \n",
        "        num_workers=FLAGS['num_workers'],\n",
        "        shuffle=True, pin_memory=True,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        datasets['valid'], batch_size=FLAGS['batch_size'], shuffle=False, \n",
        "        num_workers=FLAGS['num_workers'], drop_last=False\n",
        "    )\n",
        "\n",
        "    #train loop\n",
        "    for epoch in range(0, FLAGS['num_epochs']):\n",
        "\n",
        "        print('-'*27 + f'Epoch #{epoch+1} started' + '-'*27)\n",
        "        \n",
        "        train_loss = train_one_epoch(\n",
        "            train_loader, \n",
        "            model, optimizer, \n",
        "            scheduler=None, \n",
        "            log=log\n",
        "        )\n",
        "        \n",
        "        print(f'Average train loss for epoch #{epoch+1} : {train_loss}')\n",
        "        val_loss, f1_score = val_one_epoch(val_loader, model)\n",
        "        scheduler.step(f1_score)\n",
        "        print(f'Average val loss for epoch #{epoch+1} : {val_loss}')\n",
        "        print(f'Average f1 score for epoch #{epoch+1} : {f1_score}')\n",
        "\n",
        "        if (f1_score > best_score):\n",
        "            best_score = f1_score\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "            }, f\"{FLAGS['MODEL_PATH']}{FLAGS['model_name']}.pth\")\n",
        "\n",
        "        print('-'*28 + f'Epoch #{epoch+1} ended' + '-'*28)\n",
        "\n",
        "    best_ckpt = torch.load(f\"{FLAGS['MODEL_PATH']}{FLAGS['model_name']}.pth\")\n",
        "    model.load_state_dict(best_ckpt['model_state_dict'])\n",
        "\n",
        "    return model, best_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAgZIu3oIvZ-",
        "colab_type": "text"
      },
      "source": [
        "## Cross-val train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUWgrCvw1wJz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9c3e7ed1-8290-4281-a3e1-ebd57ddb171f"
      },
      "source": [
        "FLAGS = {}\n",
        "FLAGS['batch_size'] = 4\n",
        "FLAGS['num_workers'] = 8\n",
        "FLAGS['learning_rate'] = 1e-4\n",
        "FLAGS['num_epochs'] = 25\n",
        "FLAGS['weight_decay'] = 1e-2\n",
        "FLAGS['log_interval'] = 25\n",
        "FLAGS['img_size'] = IMG_SIZE\n",
        "FLAGS['MODEL_PATH'] = './'\n",
        "FLAGS['model_name'] = 'tf_efficientnet_b2'\n",
        "FLAGS['folds'] = 5\n",
        "\n",
        "def cross_val(folds=FLAGS['folds']): \n",
        "    \n",
        "    total_score = []\n",
        "    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
        "    skf.get_n_splits(df_train)\n",
        "\n",
        "    for fold, (train_index, val_index) in enumerate(skf.split(df_train, df_train['target'])):\n",
        "        print('='*28 + f'Fold #{fold+1} started' + '='*28)\n",
        "        trained_model, best_score = fit(df_train.loc[train_index], df_train.loc[val_index])\n",
        "        print(f'fold #{fold+1} best score: ', best_score)\n",
        "        total_score.append(best_score)\n",
        "        torch.save({\n",
        "            'model_state_dict': trained_model.state_dict()\n",
        "            }, f\"{FLAGS['MODEL_PATH']}{FLAGS['model_name']}_fold_{fold+1}.pth\")\n",
        "        print('='*28 + f'Fold #{fold+1} ended' + '='*28)\n",
        "\n",
        "    print(f'scores for all folds: {total_score}')\n",
        "    print(f'avg score over {folds} folds: {np.array(total_score).mean()}')\n",
        "\n",
        "cross_val()\n",
        "\n",
        "# ResNet18: CV 84.5277 LB 88.06930\n",
        "# EnetB0 CV 85.0458 LB 88.37636\n",
        "# EnetB2 CV 88.0906432 82.38706"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================Fold #1 started============================\n",
            "---------------------------Epoch #1 started---------------------------\n",
            "Average train loss for epoch #1 : 1.940336188224897\n",
            "Average val loss for epoch #1 : 1.614125785074736\n",
            "Average f1 score for epoch #1 : 0.5903508771929825\n",
            "----------------------------Epoch #1 ended----------------------------\n",
            "---------------------------Epoch #2 started---------------------------\n",
            "Average train loss for epoch #2 : 1.4404517673466304\n",
            "Average val loss for epoch #2 : 1.0718027259174145\n",
            "Average f1 score for epoch #2 : 0.6592105263157895\n",
            "----------------------------Epoch #2 ended----------------------------\n",
            "---------------------------Epoch #3 started---------------------------\n",
            "Average train loss for epoch #3 : 1.0280645097771737\n",
            "Average val loss for epoch #3 : 0.7815365869747964\n",
            "Average f1 score for epoch #3 : 0.7684210526315789\n",
            "----------------------------Epoch #3 ended----------------------------\n",
            "---------------------------Epoch #4 started---------------------------\n",
            "Average train loss for epoch #4 : 0.7466080090770982\n",
            "Average val loss for epoch #4 : 0.6277908588710585\n",
            "Average f1 score for epoch #4 : 0.7991228070175439\n",
            "----------------------------Epoch #4 ended----------------------------\n",
            "---------------------------Epoch #5 started---------------------------\n",
            "Average train loss for epoch #5 : 0.5142699892390264\n",
            "Average val loss for epoch #5 : 0.5462192924399125\n",
            "Average f1 score for epoch #5 : 0.8385964912280702\n",
            "----------------------------Epoch #5 ended----------------------------\n",
            "---------------------------Epoch #6 started---------------------------\n",
            "Average train loss for epoch #6 : 0.49609153156411157\n",
            "Average val loss for epoch #6 : 0.5416181871765539\n",
            "Average f1 score for epoch #6 : 0.8385964912280702\n",
            "----------------------------Epoch #6 ended----------------------------\n",
            "---------------------------Epoch #7 started---------------------------\n",
            "Average train loss for epoch #7 : 0.4694261505995711\n",
            "Average val loss for epoch #7 : 0.5229713163877788\n",
            "Average f1 score for epoch #7 : 0.8517543859649123\n",
            "----------------------------Epoch #7 ended----------------------------\n",
            "---------------------------Epoch #8 started---------------------------\n",
            "Average train loss for epoch #8 : 0.3641055463111564\n",
            "Average val loss for epoch #8 : 0.5905792556310955\n",
            "Average f1 score for epoch #8 : 0.799122807017544\n",
            "----------------------------Epoch #8 ended----------------------------\n",
            "---------------------------Epoch #9 started---------------------------\n",
            "Average train loss for epoch #9 : 0.32624941309020944\n",
            "Average val loss for epoch #9 : 0.5886095323060688\n",
            "Average f1 score for epoch #9 : 0.8254385964912282\n",
            "----------------------------Epoch #9 ended----------------------------\n",
            "---------------------------Epoch #10 started---------------------------\n",
            "Average train loss for epoch #10 : 0.38052050845876134\n",
            "Epoch    10: reducing learning rate of group 0 to 5.0000e-05.\n",
            "Average val loss for epoch #10 : 0.5496753579691837\n",
            "Average f1 score for epoch #10 : 0.8473684210526317\n",
            "----------------------------Epoch #10 ended----------------------------\n",
            "---------------------------Epoch #11 started---------------------------\n",
            "Average train loss for epoch #11 : 0.19283893311472788\n",
            "Average val loss for epoch #11 : 0.5550799495295474\n",
            "Average f1 score for epoch #11 : 0.8473684210526317\n",
            "----------------------------Epoch #11 ended----------------------------\n",
            "---------------------------Epoch #12 started---------------------------\n",
            "Average train loss for epoch #12 : 0.34864061662595563\n",
            "Average val loss for epoch #12 : 0.6043264975673274\n",
            "Average f1 score for epoch #12 : 0.7807017543859649\n",
            "----------------------------Epoch #12 ended----------------------------\n",
            "---------------------------Epoch #13 started---------------------------\n",
            "Average train loss for epoch #13 : 0.2132207425693943\n",
            "Average val loss for epoch #13 : 0.5660461218733537\n",
            "Average f1 score for epoch #13 : 0.8605263157894738\n",
            "----------------------------Epoch #13 ended----------------------------\n",
            "---------------------------Epoch #14 started---------------------------\n",
            "Average train loss for epoch #14 : 0.15929471479397114\n",
            "Average val loss for epoch #14 : 0.5539111400905409\n",
            "Average f1 score for epoch #14 : 0.8517543859649125\n",
            "----------------------------Epoch #14 ended----------------------------\n",
            "---------------------------Epoch #15 started---------------------------\n",
            "Average train loss for epoch #15 : 0.17543280425749413\n",
            "Average val loss for epoch #15 : 0.5784625536517093\n",
            "Average f1 score for epoch #15 : 0.8517543859649125\n",
            "----------------------------Epoch #15 ended----------------------------\n",
            "---------------------------Epoch #16 started---------------------------\n",
            "Average train loss for epoch #16 : 0.20831351002601728\n",
            "Epoch    16: reducing learning rate of group 0 to 2.5000e-05.\n",
            "Average val loss for epoch #16 : 0.579149625803295\n",
            "Average f1 score for epoch #16 : 0.8605263157894738\n",
            "----------------------------Epoch #16 ended----------------------------\n",
            "---------------------------Epoch #17 started---------------------------\n",
            "Average train loss for epoch #17 : 0.16101233656071637\n",
            "Average val loss for epoch #17 : 0.5950902621997031\n",
            "Average f1 score for epoch #17 : 0.8298245614035088\n",
            "----------------------------Epoch #17 ended----------------------------\n",
            "---------------------------Epoch #18 started---------------------------\n",
            "Average train loss for epoch #18 : 0.15482902026748005\n",
            "Average val loss for epoch #18 : 0.6045969756026017\n",
            "Average f1 score for epoch #18 : 0.8276315789473684\n",
            "----------------------------Epoch #18 ended----------------------------\n",
            "---------------------------Epoch #19 started---------------------------\n",
            "Average train loss for epoch #19 : 0.1435168250782849\n",
            "Epoch    19: reducing learning rate of group 0 to 1.2500e-05.\n",
            "Average val loss for epoch #19 : 0.5983570563165765\n",
            "Average f1 score for epoch #19 : 0.8254385964912281\n",
            "----------------------------Epoch #19 ended----------------------------\n",
            "---------------------------Epoch #20 started---------------------------\n",
            "Average train loss for epoch #20 : 0.14376911684258342\n",
            "Average val loss for epoch #20 : 0.557587909071069\n",
            "Average f1 score for epoch #20 : 0.8473684210526317\n",
            "----------------------------Epoch #20 ended----------------------------\n",
            "---------------------------Epoch #21 started---------------------------\n",
            "Average train loss for epoch #21 : 0.15130934160049647\n",
            "Average val loss for epoch #21 : 0.5955438739375064\n",
            "Average f1 score for epoch #21 : 0.8473684210526317\n",
            "----------------------------Epoch #21 ended----------------------------\n",
            "---------------------------Epoch #22 started---------------------------\n",
            "Average train loss for epoch #22 : 0.12063327880754862\n",
            "Epoch    22: reducing learning rate of group 0 to 6.2500e-06.\n",
            "Average val loss for epoch #22 : 0.5786584082402682\n",
            "Average f1 score for epoch #22 : 0.8517543859649123\n",
            "----------------------------Epoch #22 ended----------------------------\n",
            "---------------------------Epoch #23 started---------------------------\n",
            "Average train loss for epoch #23 : 0.1336526118525087\n",
            "Average val loss for epoch #23 : 0.5376386705197786\n",
            "Average f1 score for epoch #23 : 0.856140350877193\n",
            "----------------------------Epoch #23 ended----------------------------\n",
            "---------------------------Epoch #24 started---------------------------\n",
            "Average train loss for epoch #24 : 0.13694819302795685\n",
            "Average val loss for epoch #24 : 0.5435920420445894\n",
            "Average f1 score for epoch #24 : 0.8342105263157894\n",
            "----------------------------Epoch #24 ended----------------------------\n",
            "---------------------------Epoch #25 started---------------------------\n",
            "Average train loss for epoch #25 : 0.1042002481344628\n",
            "Epoch    25: reducing learning rate of group 0 to 3.1250e-06.\n",
            "Average val loss for epoch #25 : 0.5525240647165399\n",
            "Average f1 score for epoch #25 : 0.8342105263157894\n",
            "----------------------------Epoch #25 ended----------------------------\n",
            "fold #1 best score:  0.8605263157894738\n",
            "============================Fold #1 ended============================\n",
            "============================Fold #2 started============================\n",
            "---------------------------Epoch #1 started---------------------------\n",
            "Average train loss for epoch #1 : 1.9686076657412803\n",
            "Average val loss for epoch #1 : 1.539868662231847\n",
            "Average f1 score for epoch #1 : 0.5986842105263159\n",
            "----------------------------Epoch #1 ended----------------------------\n",
            "---------------------------Epoch #2 started---------------------------\n",
            "Average train loss for epoch #2 : 1.4956436973728546\n",
            "Average val loss for epoch #2 : 0.9982758336945584\n",
            "Average f1 score for epoch #2 : 0.6820175438596491\n",
            "----------------------------Epoch #2 ended----------------------------\n",
            "---------------------------Epoch #3 started---------------------------\n",
            "Average train loss for epoch #3 : 1.0907705491536284\n",
            "Average val loss for epoch #3 : 0.6963368983645188\n",
            "Average f1 score for epoch #3 : 0.7938596491228069\n",
            "----------------------------Epoch #3 ended----------------------------\n",
            "---------------------------Epoch #4 started---------------------------\n",
            "Average train loss for epoch #4 : 0.8046499017166765\n",
            "Average val loss for epoch #4 : 0.571789866999576\n",
            "Average f1 score for epoch #4 : 0.8092105263157894\n",
            "----------------------------Epoch #4 ended----------------------------\n",
            "---------------------------Epoch #5 started---------------------------\n",
            "Average train loss for epoch #5 : 0.6508991922417732\n",
            "Average val loss for epoch #5 : 0.5069863074704221\n",
            "Average f1 score for epoch #5 : 0.7916666666666666\n",
            "----------------------------Epoch #5 ended----------------------------\n",
            "---------------------------Epoch #6 started---------------------------\n",
            "Average train loss for epoch #6 : 0.509868565487535\n",
            "Average val loss for epoch #6 : 0.6058878004550934\n",
            "Average f1 score for epoch #6 : 0.7719298245614035\n",
            "----------------------------Epoch #6 ended----------------------------\n",
            "---------------------------Epoch #7 started---------------------------\n",
            "Average train loss for epoch #7 : 0.4278951270939553\n",
            "Epoch     7: reducing learning rate of group 0 to 5.0000e-05.\n",
            "Average val loss for epoch #7 : 0.5368594668413463\n",
            "Average f1 score for epoch #7 : 0.7916666666666667\n",
            "----------------------------Epoch #7 ended----------------------------\n",
            "---------------------------Epoch #8 started---------------------------\n",
            "Average train loss for epoch #8 : 0.3485937677834132\n",
            "Average val loss for epoch #8 : 0.537843257188797\n",
            "Average f1 score for epoch #8 : 0.7938596491228069\n",
            "----------------------------Epoch #8 ended----------------------------\n",
            "---------------------------Epoch #9 started---------------------------\n",
            "Average train loss for epoch #9 : 0.32211302404534325\n",
            "Average val loss for epoch #9 : 0.5429548809402868\n",
            "Average f1 score for epoch #9 : 0.7587719298245613\n",
            "----------------------------Epoch #9 ended----------------------------\n",
            "---------------------------Epoch #10 started---------------------------\n",
            "Average train loss for epoch #10 : 0.31483888544448435\n",
            "Average val loss for epoch #10 : 0.5185922619543577\n",
            "Average f1 score for epoch #10 : 0.831140350877193\n",
            "----------------------------Epoch #10 ended----------------------------\n",
            "---------------------------Epoch #11 started---------------------------\n",
            "Average train loss for epoch #11 : 0.31125680877737805\n",
            "Average val loss for epoch #11 : 0.49875913638817637\n",
            "Average f1 score for epoch #11 : 0.8070175438596491\n",
            "----------------------------Epoch #11 ended----------------------------\n",
            "---------------------------Epoch #12 started---------------------------\n",
            "Average train loss for epoch #12 : 0.28880837080005095\n",
            "Average val loss for epoch #12 : 0.5516465886643058\n",
            "Average f1 score for epoch #12 : 0.7653508771929823\n",
            "----------------------------Epoch #12 ended----------------------------\n",
            "---------------------------Epoch #13 started---------------------------\n",
            "Average train loss for epoch #13 : 0.23489451490036428\n",
            "Average val loss for epoch #13 : 0.5138372167160636\n",
            "Average f1 score for epoch #13 : 0.8596491228070174\n",
            "----------------------------Epoch #13 ended----------------------------\n",
            "---------------------------Epoch #14 started---------------------------\n",
            "Average train loss for epoch #14 : 0.20812143722217377\n",
            "Average val loss for epoch #14 : 0.577009191638545\n",
            "Average f1 score for epoch #14 : 0.7587719298245613\n",
            "----------------------------Epoch #14 ended----------------------------\n",
            "---------------------------Epoch #15 started---------------------------\n",
            "Average train loss for epoch #15 : 0.22302778453043062\n",
            "Average val loss for epoch #15 : 0.55166746597541\n",
            "Average f1 score for epoch #15 : 0.7938596491228069\n",
            "----------------------------Epoch #15 ended----------------------------\n",
            "---------------------------Epoch #16 started---------------------------\n",
            "Average train loss for epoch #16 : 0.1699096371151813\n",
            "Epoch    16: reducing learning rate of group 0 to 2.5000e-05.\n",
            "Average val loss for epoch #16 : 0.6510062045172641\n",
            "Average f1 score for epoch #16 : 0.7864035087719298\n",
            "----------------------------Epoch #16 ended----------------------------\n",
            "---------------------------Epoch #17 started---------------------------\n",
            "Average train loss for epoch #17 : 0.17546216579638932\n",
            "Average val loss for epoch #17 : 0.5716908244710219\n",
            "Average f1 score for epoch #17 : 0.7872807017543859\n",
            "----------------------------Epoch #17 ended----------------------------\n",
            "---------------------------Epoch #18 started---------------------------\n",
            "Average train loss for epoch #18 : 0.16149371168384813\n",
            "Average val loss for epoch #18 : 0.550473114377574\n",
            "Average f1 score for epoch #18 : 0.7916666666666665\n",
            "----------------------------Epoch #18 ended----------------------------\n",
            "---------------------------Epoch #19 started---------------------------\n",
            "Average train loss for epoch #19 : 0.21024286976619944\n",
            "Epoch    19: reducing learning rate of group 0 to 1.2500e-05.\n",
            "Average val loss for epoch #19 : 0.5601221542609366\n",
            "Average f1 score for epoch #19 : 0.7916666666666666\n",
            "----------------------------Epoch #19 ended----------------------------\n",
            "---------------------------Epoch #20 started---------------------------\n",
            "Average train loss for epoch #20 : 0.17017754953201503\n",
            "Average val loss for epoch #20 : 0.5973389446735382\n",
            "Average f1 score for epoch #20 : 0.7872807017543859\n",
            "----------------------------Epoch #20 ended----------------------------\n",
            "---------------------------Epoch #21 started---------------------------\n",
            "Average train loss for epoch #21 : 0.12038379172756247\n",
            "Average val loss for epoch #21 : 0.6053629015621386\n",
            "Average f1 score for epoch #21 : 0.7938596491228069\n",
            "----------------------------Epoch #21 ended----------------------------\n",
            "---------------------------Epoch #22 started---------------------------\n",
            "Average train loss for epoch #22 : 0.1562412050488877\n",
            "Epoch    22: reducing learning rate of group 0 to 6.2500e-06.\n",
            "Average val loss for epoch #22 : 0.5753678789264277\n",
            "Average f1 score for epoch #22 : 0.7785087719298245\n",
            "----------------------------Epoch #22 ended----------------------------\n",
            "---------------------------Epoch #23 started---------------------------\n",
            "Average train loss for epoch #23 : 0.1786219315884048\n",
            "Average val loss for epoch #23 : 0.5498713741177007\n",
            "Average f1 score for epoch #23 : 0.8070175438596491\n",
            "----------------------------Epoch #23 ended----------------------------\n",
            "---------------------------Epoch #24 started---------------------------\n",
            "Average train loss for epoch #24 : 0.0977553797196852\n",
            "Average val loss for epoch #24 : 0.5212880247517636\n",
            "Average f1 score for epoch #24 : 0.850877192982456\n",
            "----------------------------Epoch #24 ended----------------------------\n",
            "---------------------------Epoch #25 started---------------------------\n",
            "Average train loss for epoch #25 : 0.14813019478157774\n",
            "Epoch    25: reducing learning rate of group 0 to 3.1250e-06.\n",
            "Average val loss for epoch #25 : 0.5809157380932256\n",
            "Average f1 score for epoch #25 : 0.7872807017543859\n",
            "----------------------------Epoch #25 ended----------------------------\n",
            "fold #2 best score:  0.8596491228070174\n",
            "============================Fold #2 ended============================\n",
            "============================Fold #3 started============================\n",
            "---------------------------Epoch #1 started---------------------------\n",
            "Average train loss for epoch #1 : 1.9578592483311483\n",
            "Average val loss for epoch #1 : 1.6025705525749607\n",
            "Average f1 score for epoch #1 : 0.5833333333333334\n",
            "----------------------------Epoch #1 ended----------------------------\n",
            "---------------------------Epoch #2 started---------------------------\n",
            "Average train loss for epoch #2 : 1.501374844818899\n",
            "Average val loss for epoch #2 : 0.9990303328162745\n",
            "Average f1 score for epoch #2 : 0.7245614035087719\n",
            "----------------------------Epoch #2 ended----------------------------\n",
            "---------------------------Epoch #3 started---------------------------\n",
            "Average train loss for epoch #3 : 1.0645551559043258\n",
            "Average val loss for epoch #3 : 0.721743317026841\n",
            "Average f1 score for epoch #3 : 0.7859649122807016\n",
            "----------------------------Epoch #3 ended----------------------------\n",
            "---------------------------Epoch #4 started---------------------------\n",
            "Average train loss for epoch #4 : 0.7586939365488209\n",
            "Average val loss for epoch #4 : 0.6263632946892789\n",
            "Average f1 score for epoch #4 : 0.8210526315789475\n",
            "----------------------------Epoch #4 ended----------------------------\n",
            "---------------------------Epoch #5 started---------------------------\n",
            "Average train loss for epoch #5 : 0.5855855096692908\n",
            "Average val loss for epoch #5 : 0.5109974271372745\n",
            "Average f1 score for epoch #5 : 0.8649122807017544\n",
            "----------------------------Epoch #5 ended----------------------------\n",
            "---------------------------Epoch #6 started---------------------------\n",
            "Average train loss for epoch #6 : 0.4917360651370597\n",
            "Average val loss for epoch #6 : 0.5836932298384214\n",
            "Average f1 score for epoch #6 : 0.8122807017543859\n",
            "----------------------------Epoch #6 ended----------------------------\n",
            "---------------------------Epoch #7 started---------------------------\n",
            "Average train loss for epoch #7 : 0.37957631521029017\n",
            "Average val loss for epoch #7 : 0.6549519692596636\n",
            "Average f1 score for epoch #7 : 0.8078947368421053\n",
            "----------------------------Epoch #7 ended----------------------------\n",
            "---------------------------Epoch #8 started---------------------------\n",
            "Average train loss for epoch #8 : 0.41738522379365683\n",
            "Epoch     8: reducing learning rate of group 0 to 5.0000e-05.\n",
            "Average val loss for epoch #8 : 0.6079355164578086\n",
            "Average f1 score for epoch #8 : 0.8122807017543859\n",
            "----------------------------Epoch #8 ended----------------------------\n",
            "---------------------------Epoch #9 started---------------------------\n",
            "Average train loss for epoch #9 : 0.3500239263250403\n",
            "Average val loss for epoch #9 : 0.6029091320539776\n",
            "Average f1 score for epoch #9 : 0.7947368421052632\n",
            "----------------------------Epoch #9 ended----------------------------\n",
            "---------------------------Epoch #10 started---------------------------\n",
            "Average train loss for epoch #10 : 0.3000838862706537\n",
            "Average val loss for epoch #10 : 0.614546902869877\n",
            "Average f1 score for epoch #10 : 0.7859649122807018\n",
            "----------------------------Epoch #10 ended----------------------------\n",
            "---------------------------Epoch #11 started---------------------------\n",
            "Average train loss for epoch #11 : 0.2409835745618768\n",
            "Epoch    11: reducing learning rate of group 0 to 2.5000e-05.\n",
            "Average val loss for epoch #11 : 0.6118684646330381\n",
            "Average f1 score for epoch #11 : 0.8035087719298245\n",
            "----------------------------Epoch #11 ended----------------------------\n",
            "---------------------------Epoch #12 started---------------------------\n",
            "Average train loss for epoch #12 : 0.27978738734166914\n",
            "Average val loss for epoch #12 : 0.5919038794542614\n",
            "Average f1 score for epoch #12 : 0.8210526315789474\n",
            "----------------------------Epoch #12 ended----------------------------\n",
            "---------------------------Epoch #13 started---------------------------\n",
            "Average train loss for epoch #13 : 0.22107246456897423\n",
            "Average val loss for epoch #13 : 0.5834078663273862\n",
            "Average f1 score for epoch #13 : 0.8289473684210527\n",
            "----------------------------Epoch #13 ended----------------------------\n",
            "---------------------------Epoch #14 started---------------------------\n",
            "Average train loss for epoch #14 : 0.24747718523626458\n",
            "Epoch    14: reducing learning rate of group 0 to 1.2500e-05.\n",
            "Average val loss for epoch #14 : 0.5755717754364014\n",
            "Average f1 score for epoch #14 : 0.8210526315789474\n",
            "----------------------------Epoch #14 ended----------------------------\n",
            "---------------------------Epoch #15 started---------------------------\n",
            "Average train loss for epoch #15 : 0.19411716467305407\n",
            "Average val loss for epoch #15 : 0.6152655701888236\n",
            "Average f1 score for epoch #15 : 0.8122807017543859\n",
            "----------------------------Epoch #15 ended----------------------------\n",
            "---------------------------Epoch #16 started---------------------------\n",
            "Average train loss for epoch #16 : 0.20757479485991884\n",
            "Average val loss for epoch #16 : 0.5585094031534696\n",
            "Average f1 score for epoch #16 : 0.8342105263157894\n",
            "----------------------------Epoch #16 ended----------------------------\n",
            "---------------------------Epoch #17 started---------------------------\n",
            "Average train loss for epoch #17 : 0.2987711615350148\n",
            "Epoch    17: reducing learning rate of group 0 to 6.2500e-06.\n",
            "Average val loss for epoch #17 : 0.5768176725036219\n",
            "Average f1 score for epoch #17 : 0.8254385964912281\n",
            "----------------------------Epoch #17 ended----------------------------\n",
            "---------------------------Epoch #18 started---------------------------\n",
            "Average train loss for epoch #18 : 0.17979914633786842\n",
            "Average val loss for epoch #18 : 0.611017318148362\n",
            "Average f1 score for epoch #18 : 0.7991228070175439\n",
            "----------------------------Epoch #18 ended----------------------------\n",
            "---------------------------Epoch #19 started---------------------------\n",
            "Average train loss for epoch #19 : 0.2435093234765203\n",
            "Average val loss for epoch #19 : 0.6056832279029646\n",
            "Average f1 score for epoch #19 : 0.8122807017543859\n",
            "----------------------------Epoch #19 ended----------------------------\n",
            "---------------------------Epoch #20 started---------------------------\n",
            "Average train loss for epoch #20 : 0.22316191008646194\n",
            "Epoch    20: reducing learning rate of group 0 to 3.1250e-06.\n",
            "Average val loss for epoch #20 : 0.5926426696149927\n",
            "Average f1 score for epoch #20 : 0.7991228070175439\n",
            "----------------------------Epoch #20 ended----------------------------\n",
            "---------------------------Epoch #21 started---------------------------\n",
            "Average train loss for epoch #21 : 0.22935917769392875\n",
            "Average val loss for epoch #21 : 0.6026846625302967\n",
            "Average f1 score for epoch #21 : 0.8157894736842105\n",
            "----------------------------Epoch #21 ended----------------------------\n",
            "---------------------------Epoch #22 started---------------------------\n",
            "Average train loss for epoch #22 : 0.17856717354630772\n",
            "Average val loss for epoch #22 : 0.59840201390417\n",
            "Average f1 score for epoch #22 : 0.7991228070175439\n",
            "----------------------------Epoch #22 ended----------------------------\n",
            "---------------------------Epoch #23 started---------------------------\n",
            "Average train loss for epoch #23 : 0.13835960891965318\n",
            "Epoch    23: reducing learning rate of group 0 to 1.5625e-06.\n",
            "Average val loss for epoch #23 : 0.6235209766187166\n",
            "Average f1 score for epoch #23 : 0.7947368421052632\n",
            "----------------------------Epoch #23 ended----------------------------\n",
            "---------------------------Epoch #24 started---------------------------\n",
            "Average train loss for epoch #24 : 0.18296304630906615\n",
            "Average val loss for epoch #24 : 0.5850574970245361\n",
            "Average f1 score for epoch #24 : 0.8166666666666668\n",
            "----------------------------Epoch #24 ended----------------------------\n",
            "---------------------------Epoch #25 started---------------------------\n",
            "Average train loss for epoch #25 : 0.25625319482937253\n",
            "Average val loss for epoch #25 : 0.6067175802431608\n",
            "Average f1 score for epoch #25 : 0.8078947368421052\n",
            "----------------------------Epoch #25 ended----------------------------\n",
            "fold #3 best score:  0.8649122807017544\n",
            "============================Fold #3 ended============================\n",
            "============================Fold #4 started============================\n",
            "---------------------------Epoch #1 started---------------------------\n",
            "Average train loss for epoch #1 : 1.9519707242103472\n",
            "Average val loss for epoch #1 : 1.5512917230003758\n",
            "Average f1 score for epoch #1 : 0.6807017543859649\n",
            "----------------------------Epoch #1 ended----------------------------\n",
            "---------------------------Epoch #2 started---------------------------\n",
            "Average train loss for epoch #2 : 1.4679793070440423\n",
            "Average val loss for epoch #2 : 1.0120206161549217\n",
            "Average f1 score for epoch #2 : 0.8245614035087719\n",
            "----------------------------Epoch #2 ended----------------------------\n",
            "---------------------------Epoch #3 started---------------------------\n",
            "Average train loss for epoch #3 : 1.063888090114071\n",
            "Average val loss for epoch #3 : 0.6619325437043843\n",
            "Average f1 score for epoch #3 : 0.7798245614035086\n",
            "----------------------------Epoch #3 ended----------------------------\n",
            "---------------------------Epoch #4 started---------------------------\n",
            "Average train loss for epoch #4 : 0.7917347619794819\n",
            "Average val loss for epoch #4 : 0.47208849850453827\n",
            "Average f1 score for epoch #4 : 0.8609649122807018\n",
            "----------------------------Epoch #4 ended----------------------------\n",
            "---------------------------Epoch #5 started---------------------------\n",
            "Average train loss for epoch #5 : 0.6174712181091309\n",
            "Average val loss for epoch #5 : 0.4521346405932778\n",
            "Average f1 score for epoch #5 : 0.8486842105263158\n",
            "----------------------------Epoch #5 ended----------------------------\n",
            "---------------------------Epoch #6 started---------------------------\n",
            "Average train loss for epoch #6 : 0.5556605972655831\n",
            "Average val loss for epoch #6 : 0.4043138984002565\n",
            "Average f1 score for epoch #6 : 0.8881578947368421\n",
            "----------------------------Epoch #6 ended----------------------------\n",
            "---------------------------Epoch #7 started---------------------------\n",
            "Average train loss for epoch #7 : 0.49385241898771837\n",
            "Average val loss for epoch #7 : 0.3588050603866577\n",
            "Average f1 score for epoch #7 : 0.8881578947368421\n",
            "----------------------------Epoch #7 ended----------------------------\n",
            "---------------------------Epoch #8 started---------------------------\n",
            "Average train loss for epoch #8 : 0.39922856998770206\n",
            "Average val loss for epoch #8 : 0.39853053030214813\n",
            "Average f1 score for epoch #8 : 0.8596491228070177\n",
            "----------------------------Epoch #8 ended----------------------------\n",
            "---------------------------Epoch #9 started---------------------------\n",
            "Average train loss for epoch #9 : 0.3349748837621245\n",
            "Epoch     9: reducing learning rate of group 0 to 5.0000e-05.\n",
            "Average val loss for epoch #9 : 0.39341724232623454\n",
            "Average f1 score for epoch #9 : 0.8706140350877194\n",
            "----------------------------Epoch #9 ended----------------------------\n",
            "---------------------------Epoch #10 started---------------------------\n",
            "Average train loss for epoch #10 : 0.2491858361722672\n",
            "Average val loss for epoch #10 : 0.3378694151577197\n",
            "Average f1 score for epoch #10 : 0.8706140350877194\n",
            "----------------------------Epoch #10 ended----------------------------\n",
            "---------------------------Epoch #11 started---------------------------\n",
            "Average train loss for epoch #11 : 0.26435153916069903\n",
            "Average val loss for epoch #11 : 0.34866263364490707\n",
            "Average f1 score for epoch #11 : 0.8903508771929826\n",
            "----------------------------Epoch #11 ended----------------------------\n",
            "---------------------------Epoch #12 started---------------------------\n",
            "Average train loss for epoch #12 : 0.1834518019467184\n",
            "Average val loss for epoch #12 : 0.3419891972290842\n",
            "Average f1 score for epoch #12 : 0.9166666666666667\n",
            "----------------------------Epoch #12 ended----------------------------\n",
            "---------------------------Epoch #13 started---------------------------\n",
            "Average train loss for epoch #13 : 0.2265053324605504\n",
            "Average val loss for epoch #13 : 0.3834056901304345\n",
            "Average f1 score for epoch #13 : 0.875\n",
            "----------------------------Epoch #13 ended----------------------------\n",
            "---------------------------Epoch #14 started---------------------------\n",
            "Average train loss for epoch #14 : 0.322777783498168\n",
            "Average val loss for epoch #14 : 0.3663604322232698\n",
            "Average f1 score for epoch #14 : 0.8837719298245614\n",
            "----------------------------Epoch #14 ended----------------------------\n",
            "---------------------------Epoch #15 started---------------------------\n",
            "Average train loss for epoch #15 : 0.1724143025197395\n",
            "Epoch    15: reducing learning rate of group 0 to 2.5000e-05.\n",
            "Average val loss for epoch #15 : 0.3943221600432145\n",
            "Average f1 score for epoch #15 : 0.8771929824561402\n",
            "----------------------------Epoch #15 ended----------------------------\n",
            "---------------------------Epoch #16 started---------------------------\n",
            "Average train loss for epoch #16 : 0.16763645808582436\n",
            "Average val loss for epoch #16 : 0.3892487569859153\n",
            "Average f1 score for epoch #16 : 0.8662280701754387\n",
            "----------------------------Epoch #16 ended----------------------------\n",
            "---------------------------Epoch #17 started---------------------------\n",
            "Average train loss for epoch #17 : 0.22216652190848574\n",
            "Average val loss for epoch #17 : 0.40755927249004964\n",
            "Average f1 score for epoch #17 : 0.8903508771929823\n",
            "----------------------------Epoch #17 ended----------------------------\n",
            "---------------------------Epoch #18 started---------------------------\n",
            "Average train loss for epoch #18 : 0.23499258097312223\n",
            "Epoch    18: reducing learning rate of group 0 to 1.2500e-05.\n",
            "Average val loss for epoch #18 : 0.3735313101818687\n",
            "Average f1 score for epoch #18 : 0.8859649122807018\n",
            "----------------------------Epoch #18 ended----------------------------\n",
            "---------------------------Epoch #19 started---------------------------\n",
            "Average train loss for epoch #19 : 0.10505349926446399\n",
            "Average val loss for epoch #19 : 0.36505804877532155\n",
            "Average f1 score for epoch #19 : 0.9035087719298245\n",
            "----------------------------Epoch #19 ended----------------------------\n",
            "---------------------------Epoch #20 started---------------------------\n",
            "Average train loss for epoch #20 : 0.1381837737886873\n",
            "Average val loss for epoch #20 : 0.4528607396703017\n",
            "Average f1 score for epoch #20 : 0.8618421052631579\n",
            "----------------------------Epoch #20 ended----------------------------\n",
            "---------------------------Epoch #21 started---------------------------\n",
            "Average train loss for epoch #21 : 0.20641461382173512\n",
            "Epoch    21: reducing learning rate of group 0 to 6.2500e-06.\n",
            "Average val loss for epoch #21 : 0.4228818134257668\n",
            "Average f1 score for epoch #21 : 0.8859649122807018\n",
            "----------------------------Epoch #21 ended----------------------------\n",
            "---------------------------Epoch #22 started---------------------------\n",
            "Average train loss for epoch #22 : 0.16210878099480722\n",
            "Average val loss for epoch #22 : 0.4118027859612515\n",
            "Average f1 score for epoch #22 : 0.8618421052631579\n",
            "----------------------------Epoch #22 ended----------------------------\n",
            "---------------------------Epoch #23 started---------------------------\n",
            "Average train loss for epoch #23 : 0.14297369694056578\n",
            "Average val loss for epoch #23 : 0.4169967300013492\n",
            "Average f1 score for epoch #23 : 0.8881578947368421\n",
            "----------------------------Epoch #23 ended----------------------------\n",
            "---------------------------Epoch #24 started---------------------------\n",
            "Average train loss for epoch #24 : 0.19983429094291713\n",
            "Epoch    24: reducing learning rate of group 0 to 3.1250e-06.\n",
            "Average val loss for epoch #24 : 0.38128398907812017\n",
            "Average f1 score for epoch #24 : 0.8706140350877194\n",
            "----------------------------Epoch #24 ended----------------------------\n",
            "---------------------------Epoch #25 started---------------------------\n",
            "Average train loss for epoch #25 : 0.13230392949221886\n",
            "Average val loss for epoch #25 : 0.39843378412096125\n",
            "Average f1 score for epoch #25 : 0.857456140350877\n",
            "----------------------------Epoch #25 ended----------------------------\n",
            "fold #4 best score:  0.9166666666666667\n",
            "============================Fold #4 ended============================\n",
            "============================Fold #5 started============================\n",
            "---------------------------Epoch #1 started---------------------------\n",
            "Average train loss for epoch #1 : 1.9582474721621161\n",
            "Average val loss for epoch #1 : 1.5993721551365323\n",
            "Average f1 score for epoch #1 : 0.6018518518518517\n",
            "----------------------------Epoch #1 ended----------------------------\n",
            "---------------------------Epoch #2 started---------------------------\n",
            "Average train loss for epoch #2 : 1.500741179675272\n",
            "Average val loss for epoch #2 : 1.0744909577899509\n",
            "Average f1 score for epoch #2 : 0.7499999999999999\n",
            "----------------------------Epoch #2 ended----------------------------\n",
            "---------------------------Epoch #3 started---------------------------\n",
            "Average train loss for epoch #3 : 1.0508591593944863\n",
            "Average val loss for epoch #3 : 0.6940815498431524\n",
            "Average f1 score for epoch #3 : 0.8194444444444443\n",
            "----------------------------Epoch #3 ended----------------------------\n",
            "---------------------------Epoch #4 started---------------------------\n",
            "Average train loss for epoch #4 : 0.8305257077086462\n",
            "Average val loss for epoch #4 : 0.5404202573829227\n",
            "Average f1 score for epoch #4 : 0.8407407407407408\n",
            "----------------------------Epoch #4 ended----------------------------\n",
            "---------------------------Epoch #5 started---------------------------\n",
            "Average train loss for epoch #5 : 0.6709509875676404\n",
            "Average val loss for epoch #5 : 0.4547743002573649\n",
            "Average f1 score for epoch #5 : 0.8935185185185186\n",
            "----------------------------Epoch #5 ended----------------------------\n",
            "---------------------------Epoch #6 started---------------------------\n",
            "Average train loss for epoch #6 : 0.553420216253359\n",
            "Average val loss for epoch #6 : 0.5013356390926573\n",
            "Average f1 score for epoch #6 : 0.8166666666666667\n",
            "----------------------------Epoch #6 ended----------------------------\n",
            "---------------------------Epoch #7 started---------------------------\n",
            "Average train loss for epoch #7 : 0.42236684813891373\n",
            "Average val loss for epoch #7 : 0.4041064629952113\n",
            "Average f1 score for epoch #7 : 0.8842592592592592\n",
            "----------------------------Epoch #7 ended----------------------------\n",
            "---------------------------Epoch #8 started---------------------------\n",
            "Average train loss for epoch #8 : 0.4195703680384649\n",
            "Average val loss for epoch #8 : 0.4008667535252041\n",
            "Average f1 score for epoch #8 : 0.9027777777777778\n",
            "----------------------------Epoch #8 ended----------------------------\n",
            "---------------------------Epoch #9 started---------------------------\n",
            "Average train loss for epoch #9 : 0.3570867082844042\n",
            "Average val loss for epoch #9 : 0.3860452307595147\n",
            "Average f1 score for epoch #9 : 0.8703703703703705\n",
            "----------------------------Epoch #9 ended----------------------------\n",
            "---------------------------Epoch #10 started---------------------------\n",
            "Average train loss for epoch #10 : 0.25145799166535676\n",
            "Average val loss for epoch #10 : 0.4344445798132155\n",
            "Average f1 score for epoch #10 : 0.8740740740740741\n",
            "----------------------------Epoch #10 ended----------------------------\n",
            "---------------------------Epoch #11 started---------------------------\n",
            "Average train loss for epoch #11 : 0.24577841366807077\n",
            "Epoch    11: reducing learning rate of group 0 to 5.0000e-05.\n",
            "Average val loss for epoch #11 : 0.43824205464786953\n",
            "Average f1 score for epoch #11 : 0.8648148148148149\n",
            "----------------------------Epoch #11 ended----------------------------\n",
            "---------------------------Epoch #12 started---------------------------\n",
            "Average train loss for epoch #12 : 0.2093055497293603\n",
            "Average val loss for epoch #12 : 0.3504926131831275\n",
            "Average f1 score for epoch #12 : 0.8601851851851853\n",
            "----------------------------Epoch #12 ended----------------------------\n",
            "---------------------------Epoch #13 started---------------------------\n",
            "Average train loss for epoch #13 : 0.20582574034390383\n",
            "Average val loss for epoch #13 : 0.37027232514487374\n",
            "Average f1 score for epoch #13 : 0.8490740740740741\n",
            "----------------------------Epoch #13 ended----------------------------\n",
            "---------------------------Epoch #14 started---------------------------\n",
            "Average train loss for epoch #14 : 0.21028836418504585\n",
            "Epoch    14: reducing learning rate of group 0 to 2.5000e-05.\n",
            "Average val loss for epoch #14 : 0.35604294637839\n",
            "Average f1 score for epoch #14 : 0.8101851851851852\n",
            "----------------------------Epoch #14 ended----------------------------\n",
            "---------------------------Epoch #15 started---------------------------\n",
            "Average train loss for epoch #15 : 0.2200422854456183\n",
            "Average val loss for epoch #15 : 0.36767614715629154\n",
            "Average f1 score for epoch #15 : 0.8611111111111112\n",
            "----------------------------Epoch #15 ended----------------------------\n",
            "---------------------------Epoch #16 started---------------------------\n",
            "Average train loss for epoch #16 : 0.1710825456331854\n",
            "Average val loss for epoch #16 : 0.43622754514217377\n",
            "Average f1 score for epoch #16 : 0.8240740740740741\n",
            "----------------------------Epoch #16 ended----------------------------\n",
            "---------------------------Epoch #17 started---------------------------\n",
            "Average train loss for epoch #17 : 0.2261506984495137\n",
            "Epoch    17: reducing learning rate of group 0 to 1.2500e-05.\n",
            "Average val loss for epoch #17 : 0.39047712915473515\n",
            "Average f1 score for epoch #17 : 0.837962962962963\n",
            "----------------------------Epoch #17 ended----------------------------\n",
            "---------------------------Epoch #18 started---------------------------\n",
            "Average train loss for epoch #18 : 0.1852892077948949\n",
            "Average val loss for epoch #18 : 0.3985457834270265\n",
            "Average f1 score for epoch #18 : 0.8333333333333335\n",
            "----------------------------Epoch #18 ended----------------------------\n",
            "---------------------------Epoch #19 started---------------------------\n",
            "Average train loss for epoch #19 : 0.14988782226222835\n",
            "Average val loss for epoch #19 : 0.3964151094357173\n",
            "Average f1 score for epoch #19 : 0.8611111111111112\n",
            "----------------------------Epoch #19 ended----------------------------\n",
            "---------------------------Epoch #20 started---------------------------\n",
            "Average train loss for epoch #20 : 0.16048929217743546\n",
            "Epoch    20: reducing learning rate of group 0 to 6.2500e-06.\n",
            "Average val loss for epoch #20 : 0.3633630805545383\n",
            "Average f1 score for epoch #20 : 0.8685185185185186\n",
            "----------------------------Epoch #20 ended----------------------------\n",
            "---------------------------Epoch #21 started---------------------------\n",
            "Average train loss for epoch #21 : 0.17559969629326913\n",
            "Average val loss for epoch #21 : 0.3451058831479814\n",
            "Average f1 score for epoch #21 : 0.8787037037037038\n",
            "----------------------------Epoch #21 ended----------------------------\n",
            "---------------------------Epoch #22 started---------------------------\n",
            "Average train loss for epoch #22 : 0.14829424023628235\n",
            "Average val loss for epoch #22 : 0.36639823847346836\n",
            "Average f1 score for epoch #22 : 0.8333333333333335\n",
            "----------------------------Epoch #22 ended----------------------------\n",
            "---------------------------Epoch #23 started---------------------------\n",
            "Average train loss for epoch #23 : 0.15208173083932433\n",
            "Epoch    23: reducing learning rate of group 0 to 3.1250e-06.\n",
            "Average val loss for epoch #23 : 0.4131033470233281\n",
            "Average f1 score for epoch #23 : 0.837962962962963\n",
            "----------------------------Epoch #23 ended----------------------------\n",
            "---------------------------Epoch #24 started---------------------------\n",
            "Average train loss for epoch #24 : 0.13571870408646047\n",
            "Average val loss for epoch #24 : 0.3741212685902913\n",
            "Average f1 score for epoch #24 : 0.8750000000000001\n",
            "----------------------------Epoch #24 ended----------------------------\n",
            "---------------------------Epoch #25 started---------------------------\n",
            "Average train loss for epoch #25 : 0.14251619781533334\n",
            "Average val loss for epoch #25 : 0.43529530697398716\n",
            "Average f1 score for epoch #25 : 0.8611111111111112\n",
            "----------------------------Epoch #25 ended----------------------------\n",
            "fold #5 best score:  0.9027777777777778\n",
            "============================Fold #5 ended============================\n",
            "scores for all folds: [0.8605263157894738, 0.8596491228070174, 0.8649122807017544, 0.9166666666666667, 0.9027777777777778]\n",
            "avg score over 5 folds: 0.8809064327485381\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLZEN5RzI0UQ",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndklALXwMT5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset = DanceDataset(df_test, isEval=True, transform=get_valid_transform())\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset, \n",
        "    batch_size=FLAGS['batch_size'], shuffle=False, \n",
        "    num_workers=FLAGS['num_workers'], drop_last=False\n",
        ")\n",
        "\n",
        "\n",
        "model_list = []\n",
        "for i in range(0, FLAGS['folds']):\n",
        "    model = Model(FLAGS['model_name']).to(device)\n",
        "    #model = ResNet18().to(device)\n",
        "    ckpt = torch.load(f\"{FLAGS['MODEL_PATH']}{FLAGS['model_name']}_fold_{i+1}.pth\")\n",
        "    model.load_state_dict(ckpt['model_state_dict'])\n",
        "    model.eval()\n",
        "    model_list.append(model)\n",
        "\n",
        "def test(loader, model_list):\n",
        "    \n",
        "    running_loss = 0.0\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for idx, images in tqdm(enumerate(loader), total=len(loader)):\n",
        "            images = images.to(device)\n",
        "            y_pred = torch.zeros((FLAGS['batch_size'], le.classes_.shape[0]), dtype=torch.float)\n",
        "            for model in model_list:\n",
        "                y_pred += model(images).float().cpu()\n",
        "            predictions.append(y_pred.argmax(1).numpy())\n",
        "    return le.inverse_transform(np.hstack(predictions))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MsE1__dI2x-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "1d13ff0d528c48af956330ad1791433a",
            "be697b20e6be403592a2932ff6ea1c73",
            "c32d21e932614a11ae29e17ae0927094",
            "61d6cb7117884acba4a30bcaf4696f18",
            "67e9662bcf0a4588a0b350e122520c56",
            "7ffa1cecd69d40cc83edff9e4ed6b32f",
            "2751ada90461401187102e0efbf378c4",
            "d13c6c1ae6e5479c807aae850be709f2"
          ]
        },
        "outputId": "180b609d-6e71-48f3-883e-3606397568a0"
      },
      "source": [
        "df_test['target'] = test(test_dataloader, model_list)\n",
        "df_test.to_csv('submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d13ff0d528c48af956330ad1791433a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=39.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjXDrfgoOhtW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "c5e8c665-98ff-4772-9dd1-e6594a0ceea7"
      },
      "source": [
        "df_test['target'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odissi           27\n",
              "kathak           27\n",
              "sattriya         21\n",
              "kathakali        19\n",
              "kuchipudi        19\n",
              "mohiniyattam     18\n",
              "bharatanatyam    17\n",
              "manipuri          8\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}